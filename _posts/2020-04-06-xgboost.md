---
title: "Predicting Housing Contract Cancellation using Machine Learning"
classes: wide
tags: [python, machine learning, xgboost]
desc: "Predicting housing contract cancellation with XGBoost"
---


I've been learning about applying different machine learning algorithms for a while now, through my courses and other learning content on the web. But this is the first pretty good example of applying machine learning to a business problem at work, so I am excited to share it with you!

I was given the charge to build a model to use as justification to share (or not to share) financial aid-related data with the housing department. Basically, the housing department wants to be able to make informed decisions and to build their own model to predict whether or not a student will cancel their housing contract, because the cancellations are involved in how they manage their waitlist process. They are requesting that they get access to some student-level financial aid data in order to do this. But where I work, granular-level data is only shared sparingly between departments. This is where this model comes into play; there has to be justification that the financial aid information is important **enough** to share for this purpose.

To solve this problem, I wanted to build a model using all the data and compare that to the same model fitted *without* the financial aid features. The most recent machine learning model I learned was XGBoost, so I wanted to give that a shot. This is a relatively new model that combines gradient descent and random forests that has been getting good performance in data science competitions, so I am keen to practice this library more and more. I really like decision trees and random forests because they are intuitive and easy to understand, so gradient-boosted random forests will probably be amongst my favorite models to use!

This is a binary classification task, so the model will be predicting cancellation/no cancellation of the housing contract. The other element to this problem that is unique is that I decided to do two separate models: one for new incoming students and then one for returning students. This is due to the fact that there are different features available for returning students.

The two features that are financial aid-related are how much need the student had, and whether or not the student received a housing-related award. The other features that I chose to use are the student's primary county and how many months before move-in the contract was signed. When I go through the functions that I created, I will explain the features in more detail in order to explain why I chose it as a feature in the model. Let's get into the jupyter notebook!

For this jupyter notebook, I imported the typical packages that I usually do, in addition to the XGBoost package.


```python
#import packages
%matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import xgboost as xgb
```

This next cell is where I imported all the data for this project. The housing department provides the data related to the housing contract, and then I went ahead and pulled data from the database related to the other features that I need. I have them all in excel files, so I will use the built-in pandas function to read them in. Let's take a look so you can see the format that the data is in.


```python
#select files after upload and put them in dfs
df1 = pd.read_excel('070119 F18 Housing Contract Report.xlsx')
df2 = pd.read_excel('101419 F19 Housing Contract Report.xlsx')
stuinfo = pd.read_excel('student info.xlsx')
awinfo = pd.read_excel('award info.xlsx')
needinfo = pd.read_excel('need info.xlsx')
```


```python
df1_c = df1.copy()
df1_c.loc[:, ['Student Number','First Name', 'Last Name', 'Email']] = 'Hidden'
```


```python
df1_c.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>TERM</th>
      <th>Student Number</th>
      <th>First Name</th>
      <th>Last Name</th>
      <th>Email</th>
      <th>Application Received Timestamp</th>
      <th>Application Canceled Timestamp</th>
      <th>SIGNATURE</th>
      <th>ASSIGNED</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>Y</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>Y</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>Y</td>
      <td>N</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>Y</td>
      <td>Y</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>Y</td>
      <td>Y</td>
    </tr>
  </tbody>
</table>
</div>



This first dataset is an example of what the housing department provides as far as the housing contracts. The "Application Canceled Timestamp" column designates when the contract was cancelled, so that will indicate a cancellation. The other columns are straightforward, but I will only use the term, the "Application Received Timestamp", and the "Application Canceled Timestamp". There are two datasets like this; the other is for the 201940 term (i.e. the following year). I will append these two later to create one dataset with all of the contracts.


```python
stuinfo_c = stuinfo.copy()
stuinfo_c.loc[:,['PIDM','GID']] = "Hidden"
```


```python
stuinfo_c.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PIDM</th>
      <th>GID</th>
      <th>TERM</th>
      <th>CNTY</th>
      <th>COLL</th>
      <th>STYP</th>
      <th>CLAS</th>
      <th>OV_GPA</th>
      <th>TGPA</th>
      <th>PREV_TGPA</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>201840</td>
      <td>Macomb</td>
      <td>School of Nursing</td>
      <td>First-time Graduate</td>
      <td>Graduate Certificate</td>
      <td>4.00</td>
      <td>NaN</td>
      <td>4.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>201633</td>
      <td>Genesee</td>
      <td>All Colleges</td>
      <td>Continuing Education</td>
      <td>Continuing Education</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>201840</td>
      <td>Oakland</td>
      <td>University Programs</td>
      <td>Returning FTIAC</td>
      <td>Senior</td>
      <td>2.42</td>
      <td>3.3</td>
      <td>3.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>201910</td>
      <td>Oakland</td>
      <td>University Programs</td>
      <td>Returning FTIAC</td>
      <td>Senior</td>
      <td>2.47</td>
      <td>3.3</td>
      <td>3.3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Hidden</td>
      <td>Hidden</td>
      <td>201940</td>
      <td>Oakland</td>
      <td>University Programs</td>
      <td>Returning FTIAC</td>
      <td>Senior</td>
      <td>2.50</td>
      <td>3.0</td>
      <td>3.3</td>
    </tr>
  </tbody>
</table>
</div>



This dataset is typical student information; for every term, a student will have a record with this information. For the purposes of this model, in that we are focusing on newly admitted students, the only column we will need is the "CNTY" column. This is the county of that student's primary address.


```python
awinfo_c = awinfo.copy()
awinfo_c.loc[:,['PIDM']] = "Hidden"
```


```python
awinfo_c.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PIDM</th>
      <th>TERM</th>
      <th>FUND</th>
      <th>AWST</th>
      <th>TITLE</th>
      <th>OFFER_AMT</th>
      <th>PAID_AMT</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Hidden</td>
      <td>201610</td>
      <td>OUGRNT</td>
      <td>CNCL</td>
      <td>OU Housing Grant</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hidden</td>
      <td>201610</td>
      <td>OUGRNT</td>
      <td>ACPT</td>
      <td>OU Housing Grant</td>
      <td>2500.0</td>
      <td>2500.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Hidden</td>
      <td>201610</td>
      <td>OUGRNT</td>
      <td>CNCL</td>
      <td>OU Housing Grant</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hidden</td>
      <td>201610</td>
      <td>OUGRNT</td>
      <td>ACPT</td>
      <td>OU Housing Grant</td>
      <td>2500.0</td>
      <td>2500.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Hidden</td>
      <td>201610</td>
      <td>OUGRNT</td>
      <td>CNCL</td>
      <td>OU Housing Grant</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



This dataset is a a table of award data. Every row is an award offered; I will reference this table later to determine if the student had a housing award for the model.


```python
needinfo_c = needinfo.copy()
needinfo_c.loc[:,['PIDM']] = "Hidden"
```


```python
needinfo_c.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PIDM</th>
      <th>AIDY</th>
      <th>NEED</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Hidden</td>
      <td>1819</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Hidden</td>
      <td>1819</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Hidden</td>
      <td>1819</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Hidden</td>
      <td>1819</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Hidden</td>
      <td>1819</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



This simple dataset has the amount of need that a student has for every term. We will use the amount of need as one of our features for the model.

Now that we've gone through the datasets, let's continue on with the project. The next cell is where I will combine the two housing contract datasets vertically into one dataset.


```python
#combine housing records into one df
df3 = pd.concat([df1,df2], ignore_index=True)
```

Next, I picked out the relevant columns and then renamed/changed the column names to lowercase.


```python
#choose relevant columns
df3 = df3[['TERM','Student Number', 'Application Received Timestamp',
           'Application Canceled Timestamp']]

#change column names to lowercase
df3.columns = ['housing_term','gid','app_rec_date','app_cnl_date']
stuinfo.columns = [i.lower() for i in stuinfo.columns]
awinfo.columns = [i.lower() for i in awinfo.columns]
needinfo.columns = [i.lower() for i in needinfo.columns]
```

Next, I will create the column to use for the model target. Using the lambda function on the cancelled date column, it creates a column with a 1 if the column has a date and then a 0 otherwise. I will apply the function and then print out an example so that you can see the result, but I will drop the GID column before printing it


```python
#create target column from the app_cnl_date column
df3['cancelled'] = df3.app_cnl_date.apply(lambda x: 1 if x > pd.to_datetime('1/1/2000') else 0)
```


```python
df3.drop('gid', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



The next section of cells is where I did a lot of preprocessing to the data. This section is not particularly cool or interesting, but it is the bulk of the work. Feel free to skip ahead if you are just interested in the model fitting and evaluation. But I think it's worth checking out how I prepared the data, and if you have any tips or feedback, please let me know!

In any case, the following cell is where I defined the functions to get the term of when the student signed the housing contract and the term of when the contract was cancelled. The functions are pretty straightforward, they check the input value of the date and then based on the date, it returns the proper term.

After those definitions, I applied the functions to the relevant columns to create the two new columns.


```python
#function to get the signup term
def get_term(x):
  if pd.to_datetime('1/1/2018') <= x <= pd.to_datetime('7/31/2018'):
    return '201810'
  if pd.to_datetime('7/31/2018') < x <= pd.to_datetime('12/31/2018'):
    return '201840'
  if pd.to_datetime('12/31/2018') < x <= pd.to_datetime('7/31/2019'):
    return '201910'
  else:
    return '201940'

#function to get the cancel term
def get_cancel_term(x):
  if pd.to_datetime('1/1/2018') <= x <= pd.to_datetime('7/31/2018'):
    return '201810'
  if pd.to_datetime('7/31/2018') < x <= pd.to_datetime('12/31/2018'):
    return '201840'
  if pd.to_datetime('12/31/2018') < x <= pd.to_datetime('7/31/2019'):
    return '201910'
  if x > pd.to_datetime('7/31/2019'):
    return '201940'
  else:
    return 'NA'

#apply functions
df3['signup_term'] = df3.app_rec_date.apply(get_term)
df3['cancel_term'] = df3.app_cnl_date.apply(get_cancel_term)
```


```python
df3.drop('gid', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
    </tr>
  </tbody>
</table>
</div>



The next function takes the gid and the term of the input row, and then refers to the "stuinfo" dataset to get the student type of the student at the time of housing signup. Since new students would not have a record in the stuinfo dataset, the function checks for that and then returns "New Student" as a string.

Once again, I will apply the function to the dataframe. In addition though, I had to apply another function to make sure that the values in the new column are strings. This is because the first function that I wrote would return the student type within a list, so this second (lambda) function takes care of that little issue.


```python
#function to get styp at signup
def get_signup_styp(row):
  stu = row['gid']
  term = int(row['signup_term'])
  styp = stuinfo.loc[(stuinfo.term == term) & (stuinfo.gid == stu), 'styp'].values
  if len(styp) == 1:
    return styp
  else:
    return "New Student"

#apply function and change it to str
df3['signup_styp'] = df3.apply(get_signup_styp,axis=1)
df3['signup_styp'] = df3.signup_styp.apply(lambda x: x if type(x) == str else x[0]).astype('str')
```


```python
df3.drop('gid', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
    </tr>
  </tbody>
</table>
</div>



The next two functions simply get the pidm and county respectively, by using the gid of the input row. One thing I need to draw your attention to is the second function to the county column. Essentially what I did was create a binary column indicating whether or not the student lives in the Tri-county area in Michigan. I chose to do this because it reduces the cardinality of this feature to down to a binary, which for our purposes is enough. For example, a student that lives in the area is more likely to be able to commute, which could be telling for a student that is thinking about cancelling their housing contract. This is the basic idea behind why I chose to engineer the feature in this way.


```python
#function to get pidm
def get_pidm(row):
  stu = row['gid']
  pidm = stuinfo.loc[stuinfo.gid == stu, 'pidm']
  if len(pidm) > 0:
    return pidm.iat[0]
  else:
    return "No enrollment info"

#function to get cnty
def get_cnty(row):
  stu = row['gid']
  cnty = stuinfo.loc[stuinfo.gid == stu, 'cnty']
  if cnty.size == 0:
    return 'NA'
  else:
    return cnty.iat[0]

#apply function
df3['cnty'] = df3.apply(get_cnty, axis=1)
df3['cnty'] = df3.cnty.apply(lambda x: "Tri-County" if x in ["Macomb", "Oakland", "Wayne"] else "Other").astype("category")

#apply function and drop gid
df3['pidm'] = df3.apply(get_pidm, axis=1)
df3.drop('gid', axis=1, inplace=True)
stuinfo.drop('gid', axis=1, inplace=True)
```


```python
df3.drop('pidm', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
      <th>cnty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
      <td>Other</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
    </tr>
  </tbody>
</table>
</div>



The next function takes the pidm and housing term and checks for whether or not the student had a housing award.


```python
#function to check for housing award
def get_award(row):
  stu = row['pidm']
  term = row['housing_term']
  awards = awinfo.loc[(awinfo.pidm == stu) & (awinfo.term == term), 'title']
  if len(awards) > 0:
    return awards.iat[0]
  else:
    return "No award"

#apply function
df3['award'] = df3.apply(get_award, axis=1)
```

    /Users/andy/anaconda3/lib/python3.7/site-packages/pandas/core/ops/array_ops.py:253: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
      res_values = method(rvalues)



```python
df3.drop('pidm', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
      <th>cnty</th>
      <th>award</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
      <td>Other</td>
      <td>No award</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
    </tr>
  </tbody>
</table>
</div>



In this next code block, I created three new columns. The first is for the number of months the contract was live, the next is to create a generic move-in date, and the final column is to subtract the move-in date and the date of signup to determine how many months ahead of the move-in date that the student signed up for housing. I chose this feature because I was wondering if the students that signed up very early were more likely to cancel.


```python
#calculate # of months contract was active for
df3['months_app_live'] = round((df3.app_cnl_date - df3.app_rec_date) / np.timedelta64(1, 'M'),0)

#create column for move-in date
df3['movein_date'] = pd.to_datetime(df3.housing_term.apply(lambda x: "9/1/2018" if x == 201840 else "9/1/2019"))

#create column for number of months before move-in at time of sign-up
df3["sign_ahead"] = round((df3.movein_date - df3.app_rec_date) / np.timedelta64(1, 'M'),0)
```


```python
df3.drop('pidm', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
      <th>cnty</th>
      <th>award</th>
      <th>months_app_live</th>
      <th>movein_date</th>
      <th>sign_ahead</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>3.0</td>
      <td>2018-09-01</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
      <td>Other</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
    </tr>
  </tbody>
</table>
</div>



Now that I have the number of months ahead of move-in, I created a function to categorize them. Because tree-based models have a tendency to overfit, I am creating this category to reduce the number of possible options while still capturing the overall idea. I created three categories which you can see in the code below.


```python
#function for months signed up ahead of move-in category
def sign_ahead_cat(x):
  if x < 4:
    return "< 4 mos"
  elif x < 8:
    return "4 - 7 mos"
  else:
    return "8+ mos"

#apply function
df3["sign_ahead_cat"] = df3.sign_ahead.apply(sign_ahead_cat).astype("category")
```


```python
df3.drop('pidm', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
      <th>cnty</th>
      <th>award</th>
      <th>months_app_live</th>
      <th>movein_date</th>
      <th>sign_ahead</th>
      <th>sign_ahead_cat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>3.0</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
      <td>Other</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
    </tr>
  </tbody>
</table>
</div>



The next feature that I will need to bring into the dataset is the amount of need that the student had. To do this, I will have to have the term in the "needinfo" dataset in order to join the two datasets; right now, it only has the aid year (e.g. "1819", "1920", etc). Luckily, there are only two options, so I wrote a lambda function to create a column with the fall terms for that aid year, which is enough to do the join. The second line of the following code block details the join of the needinfo; I used the pidm and the housing term to join it.


```python
#get term from aidy in the needinfo table
needinfo['term'] = needinfo.aidy.apply(lambda x: 201840 if x == 1819 else 201940 )

#merge dfs to bring in need information
df3 = pd.merge(df3, needinfo, how = "left", left_on= ["pidm", "housing_term"], right_on= ["pidm", "term"])
```


```python
df3.drop('pidm', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
      <th>cnty</th>
      <th>award</th>
      <th>months_app_live</th>
      <th>movein_date</th>
      <th>sign_ahead</th>
      <th>sign_ahead_cat</th>
      <th>aidy</th>
      <th>need</th>
      <th>term</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>0.0</td>
      <td>201840.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>758.0</td>
      <td>201840.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>3.0</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>29895.0</td>
      <td>201840.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
      <td>Other</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>3438.0</td>
      <td>201840.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>0.0</td>
      <td>201840.0</td>
    </tr>
  </tbody>
</table>
</div>



Now that I have the need amount in the dataset, I am going to combine this feature into a categorical. I used the quartiles as a guide to split it into categories; once again, this will assist in preventing potential overfitting.


```python
#function for need category
def need_cat(x):
  if x == 0:
    return "None"
  elif x <= df3.loc[df3.need > 0].need.quantile(.25):
    return "Low"
  elif x <= df3.loc[df3.need > 0].need.quantile(.5):
    return "Moderate"
  elif x <= df3.loc[df3.need > 0].need.quantile(.75):
    return "High"
  else:
    return "Very high"

#apply function
df3["need_cat"] = df3.need.apply(need_cat)
```


```python
df3.drop('pidm', axis = 1).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>housing_term</th>
      <th>app_rec_date</th>
      <th>app_cnl_date</th>
      <th>cancelled</th>
      <th>signup_term</th>
      <th>cancel_term</th>
      <th>signup_styp</th>
      <th>cnty</th>
      <th>award</th>
      <th>months_app_live</th>
      <th>movein_date</th>
      <th>sign_ahead</th>
      <th>sign_ahead_cat</th>
      <th>aidy</th>
      <th>need</th>
      <th>term</th>
      <th>need_cat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201840</td>
      <td>2018-02-01 08:03:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>0.0</td>
      <td>201840.0</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201840</td>
      <td>2018-02-01 08:04:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>758.0</td>
      <td>201840.0</td>
      <td>Low</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>2018-05-02 05:14:00</td>
      <td>1</td>
      <td>201810</td>
      <td>201810</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>3.0</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>29895.0</td>
      <td>201840.0</td>
      <td>High</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>First-time FTIAC</td>
      <td>Other</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>3438.0</td>
      <td>201840.0</td>
      <td>Low</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201840</td>
      <td>2018-02-01 08:05:00</td>
      <td>NaT</td>
      <td>0</td>
      <td>201810</td>
      <td>NA</td>
      <td>Returning FTIAC</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>NaN</td>
      <td>2018-09-01</td>
      <td>7.0</td>
      <td>4 - 7 mos</td>
      <td>1819.0</td>
      <td>0.0</td>
      <td>201840.0</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div>



So with that, I have all the features I need to train the XGBoost model! I will do just a few more things to prepare the data, and then I will go into training the model and cross validation.

First, I will need to filter the dataset to only include new students. Remember, this is one of the models that I built for this problem; one for the new students and then one for returning students.


```python
new_students = df3.loc[df3.signup_styp == "New Student", ["cancelled", "cnty", "award",
                                                          "sign_ahead_cat", "need_cat"]]
```


```python
new_students.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cancelled</th>
      <th>cnty</th>
      <th>award</th>
      <th>sign_ahead_cat</th>
      <th>need_cat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>338</th>
      <td>0</td>
      <td>Other</td>
      <td>OU Housing Grant Renewal</td>
      <td>4 - 7 mos</td>
      <td>High</td>
    </tr>
    <tr>
      <th>899</th>
      <td>0</td>
      <td>Other</td>
      <td>No award</td>
      <td>4 - 7 mos</td>
      <td>None</td>
    </tr>
    <tr>
      <th>913</th>
      <td>0</td>
      <td>Other</td>
      <td>No award</td>
      <td>4 - 7 mos</td>
      <td>Very high</td>
    </tr>
    <tr>
      <th>1093</th>
      <td>0</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>4 - 7 mos</td>
      <td>None</td>
    </tr>
    <tr>
      <th>1801</th>
      <td>0</td>
      <td>Tri-County</td>
      <td>No award</td>
      <td>4 - 7 mos</td>
      <td>High</td>
    </tr>
  </tbody>
</table>
</div>



Now that I have the right population, I have to split up the data into the training features and target column. First, I will create a new object, X, with the training features. I will encode the award column to be binary: 1 or 0. Then, I take X and use the "get_dummies" function of pandas, which will go through and create one-hot encoded columns from the categorical features that I have.


```python
X = new_students.drop('cancelled',axis=1)
X["award"] = X.award.apply(lambda x: 0 if x == "No award" else 1)
X = pd.get_dummies(X)
```


```python
X.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>award</th>
      <th>cnty_Other</th>
      <th>cnty_Tri-County</th>
      <th>sign_ahead_cat_4 - 7 mos</th>
      <th>sign_ahead_cat_8+ mos</th>
      <th>sign_ahead_cat_&lt; 4 mos</th>
      <th>need_cat_High</th>
      <th>need_cat_Low</th>
      <th>need_cat_Moderate</th>
      <th>need_cat_None</th>
      <th>need_cat_Very high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>338</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>899</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>913</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1093</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1801</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



From the dataset printed above, you will see that the "get_dummies" function created all the necessary binary columns. This will allow the XGBoost package to work, as it needs numerical values for the categorical variables.

Finally, I will extract the target feature (a.k.a. housing cancellation) into an object, y.


```python
y = new_students[['cancelled']]
```

Now I am ready to train the XGBoost model! First, I will instantiate the model and then rename the columns so that the column names are able to be read by the model; I believe that you can only have certain non-alphanumeric characters.


```python
clf = xgb.XGBClassifier()

X.columns = ['award', 'cnty_Other', 'cnty_Tri-County', 'sign_ahead_cat_4 thru 7 mos',
       'sign_ahead_cat_8 plus mos', 'sign_ahead_cat_less than 4 mos', 'need_cat_High',
       'need_cat_Low', 'need_cat_Moderate', 'need_cat_None',
       'need_cat_Very high']
```


```python
clf
```




    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                  colsample_bynode=1, colsample_bytree=1, gamma=0,
                  learning_rate=0.1, max_delta_step=0, max_depth=3,
                  min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,
                  nthread=None, objective='binary:logistic', random_state=0,
                  reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
                  silent=None, subsample=1, verbosity=1)



The next thing I will do is to create what is called a DMatrix from the data that we have. This is a specific data structure that is included in the XGBoost package. I am interested but don't know much about it beyond that, but I need to use it to train the model. After that, I created a dictionary with the parameters for the objective function and the max depth of the individual decision trees. For now, I am keeping it simple with the parameters; I can and perhaps will go back at some point and change more of the hyperparameters with grid search cross validation.


```python
dmat = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:logistic", "max_depth":3}
```

Now that I have the training data set and the model instantiated, I will now go ahead and run cross validation. In short, what it's going to do is split the data three times, creating a training set and testing set. For these three times, the model will evaluate and then provide the evaluation summary statistics. Since we are doing the default number of boosting rounds, you will see that it prints out 10 rows. Therefore, each row is a boosting round, and the statistics are *summaries* of the 3 folds. The metrics that I decided to use for evaluation were the "roc auc" score and the standard error, which is 1 - accuracy.


```python
cv = xgb.cv(params=params, dtrain = dmat, as_pandas=True, metrics=['auc', 'error'], nfold=3, stratified=True)
```


```python
cv
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train-auc-mean</th>
      <th>train-auc-std</th>
      <th>train-error-mean</th>
      <th>train-error-std</th>
      <th>test-auc-mean</th>
      <th>test-auc-std</th>
      <th>test-error-mean</th>
      <th>test-error-std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.859566</td>
      <td>0.019712</td>
      <td>0.079183</td>
      <td>0.001848</td>
      <td>0.851132</td>
      <td>0.008731</td>
      <td>0.079821</td>
      <td>0.002747</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.871919</td>
      <td>0.006104</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.868466</td>
      <td>0.011552</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.872229</td>
      <td>0.006262</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.868703</td>
      <td>0.011584</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.873585</td>
      <td>0.004017</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.868567</td>
      <td>0.011956</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.875257</td>
      <td>0.002788</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.870675</td>
      <td>0.013117</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.876469</td>
      <td>0.003566</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.870087</td>
      <td>0.012987</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.877333</td>
      <td>0.003668</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.871153</td>
      <td>0.012644</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.878776</td>
      <td>0.004739</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.874199</td>
      <td>0.010797</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.879496</td>
      <td>0.005607</td>
      <td>0.080460</td>
      <td>0.001410</td>
      <td>0.874443</td>
      <td>0.009888</td>
      <td>0.080460</td>
      <td>0.002820</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.879721</td>
      <td>0.005735</td>
      <td>0.079662</td>
      <td>0.002154</td>
      <td>0.874446</td>
      <td>0.010747</td>
      <td>0.080140</td>
      <td>0.002389</td>
    </tr>
  </tbody>
</table>
</div>



If you look at the test-auc-mean and test-error-mean, this is the mean score for the two evaluation metrics on the test set. For the final boosting round, the mean error was ~0.08 or ~92% accuracy. The mean auc score was ~0.87. All in all, for as simple of a model that this is, that's not too bad. If you also take a look at the difference in the evaluation for the test and training sets, there isn't too much of a difference, but a slight overfit. You can tell because the error is slightly higher for the test set, but it's very small. Overall, this went pretty well, I think!

Now that we did cross validation, let's train the model so that we can print out the feature importances. This is finally how we will evaluate if the financial aid related data is important. I will import the "train_test_split" function from sklearn as well as the "accuracy_score". This will allow me to split the data into train and test sets and then to evaluate once I train the model.


```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = .25)
```

Now the data is split and ready for training. I will use the sklearn-compatible API that XGBoost provides, which is really nice. I will train the model and then test it using our test set, using the overall accuracy as the metric here.


```python
clf.fit(X_train, np.ravel(y_train))
```




    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
                  colsample_bynode=1, colsample_bytree=1, gamma=0,
                  learning_rate=0.1, max_delta_step=0, max_depth=3,
                  min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,
                  nthread=None, objective='binary:logistic', random_state=0,
                  reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
                  silent=None, subsample=1, verbosity=1)




```python
predictions = clf.predict(X_test)
print("The accuracy score is: ", accuracy_score(y_test, predictions))
```

    The accuracy score is:  0.9272030651340997


As we expected from what we saw in cross validation, the accuracy is upwards of 90%. Now I will have use the plot_importance function provided in the XGBoost model. This shows you how many times the column was used as a split point by the individual base trees in the model. The more times it was used as a split point, the more *important* that feature is.


```python
xgb.plot_importance(clf)
```




<img src="{{ site.url }}{{ site.baseurl }}/images/xgboost/f1.png" alt="">


From the plot, it tells us that the most important feature by some distance is the county feature. But the award and need columns were used decently well, too. The drawback to using a model like this is that it's not easy to see the details of the split points and nodes; after all, there are 100 base decision tree estimators. You can only see from this *which* columns the model chose to use, not *how* they were used. But in any case, I will continue on and take out the financial aid related data.

I will drop the related columns and then train and test the model. Finally, I will print out the feature importances and then evaluate the difference between the accuracy of the model with and without the financial aid data.


```python
X_train2 = X_train.drop(['need_cat_High', 'need_cat_Low', 'need_cat_Moderate', 'need_cat_None',
       'need_cat_Very high', 'award'], axis=1)
X_test2 = X_test.drop(['need_cat_High', 'need_cat_Low', 'need_cat_Moderate', 'need_cat_None',
       'need_cat_Very high', 'award'], axis=1)
```


```python
clf.fit(X_train2, np.ravel(y_train))
predictions2 = clf.predict(X_test2)
print("The accuracy score is: ", accuracy_score(y_test, predictions2))
```

    The accuracy score is:  0.8326947637292464



```python
xgb.plot_importance(clf)
```


<img src="{{ site.url }}{{ site.baseurl }}/images/xgboost/f2.png" alt="">



```python
print("The difference in accuracy is: ", round(accuracy_score(y_test, predictions) - accuracy_score(y_test, predictions2),2))
```

    The difference in accuracy is:  0.09


After re-running the model without the financial aid data, we lost almost 10% of accuracy. Interestingly, the feature importances changed in a way I didn't expect, with the "months signed up ahead of move-in" column being the most highly used.

Based on this "experiment", I would say that the financial aid data is important to include for the model. You take quite a hit in accuracy without it. This could be that there just isn't as much relevant information for new students outside of these, compared to returning students.

Thank you very much for taking the time to read through this post! As always, I always enjoy feedback and tips, or any thoughts you have about this in general. I thought this was nice way to hone applying machine learning, and I had a lot of fun working my way through this problem. I hope you enjoyed it and please take care!
